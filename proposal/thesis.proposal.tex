\documentclass{report}
\usepackage[top=3.5cm, left=3cm, right=3cm]{geometry}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{titlesec}

% contents links
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

\begin{document}
\titleformat{\chapter}{}{}{0em}{\bf\LARGE}
\pagenumbering{gobble}

% This is the title page.

\centerline{\Huge Guest-Based System Call Introspection}
\vspace{3mm}
\centerline{\Huge with Extended Berkeley Packet Filter}
\vspace{14mm}
\centerline{\large by}
\vspace{15mm}
\centerline{\itshape \large Huzaifa Patel}
\vspace{2cm}
\centerline{\large A thesis proposal submitted to the School of Computer Science in partial fulfillment}
\vspace{2mm}
\centerline{\large of the requirements for the degree of}
\vspace{2cm}
\centerline{\bf \large Bachelor of Computer Science}
\vspace{3cm}
\centerline{\large Under the supervision of Dr. Anil Somayaji}
\vspace{3mm}
\centerline{\large Carleton University}
\vspace{3mm}
\centerline{\large Ottawa, Ontario}
\vspace{3mm}
\centerline{\large September, 2022}
\vspace{3cm}
\centerline{\large \copyright \: 2022 Huzaifa Patel}


% Quote Page.

\newpage
\pagebreak
\hspace{0pt}
\vfill
\centerline{\itshape \large their kindness is masquerade.}
\vspace{3mm}
\centerline{\itshape \large yearning to occupy one with false pretenses.}
\vspace{3mm}
\centerline{\itshape \large it's used to sedate.}
\vspace{3mm}
\centerline{\itshape \large I promise you'll get this when the sky clears for you.}
\vfill
\hspace{0pt}
\pagebreak


\begin{spacing}{1.5}









% Abstract.


\newpage
\pagenumbering{roman}
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}















% Acknowledgments.

\newpage

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}

{\large I want to express my heartfelt gratitude to my supervisor, Dr. Anil Somayaji for providing me with the opportunity to work on a thesis during the final year of my undergraduate degree. Unlike previous variations of the Computer Science undergraduate degree requirements, completing a thesis is no longer a prerequisite. Therefore, I prostualte it is a great privlidge and honor to be given the opportunity to enroll into a thesis-based course during ones undergraduate studies.}

{\large I did not have prior experience in formal research when I first approached Dr. Somayaji. Despite this shortcoming, it did not stop him from investing his time and resources towards my academic growth. Without his feedback and ideas on my framework implementation and writing of this thesis, as well as his expertise in eBPF, Hypervisors, and Unix based Operating Systems, this thesis would not have been possible.}

{\large I would like to commend PhD student Manuel Andreas from The Technical University of Munich for introducing me to the concept of a Hypervisor. Without him, I would not have approached Dr. Somayaji with the intention of wanting to conduct research on them. His minor action of introducing me to hypervisors had the significant effect of inspiring me to write a thesis on the subject. I also want to thank him for his willingness to endlessly and tirelessly teach, discuss and help me understand the intricacies of hypervisors, the Linux kernel, and the C programming language.} 

{\large I would also like to thank Carleton University's faculty of Computer Science for their efforts in imparting knowledge that has enthraled and inspired me to learn all that I can about Computer Science.}

{\large I would like to extend my appreciation to the various internet communities which have provided the world with invaluable compiled resources on hypervisors, Unix based operating systems, eBPF, the Linux kernel, the C programming language, and Latex, which has helped me tremendously in writing this thesis.}

{\large Finally, I would like to thank my immediate family for their encouragement and support towards my research interests and educational pursuits.}

% define table of contents
\tableofcontents











% Nomenclature

\newpage
\chapter*{Nomenclature}
\addcontentsline{toc}{chapter}{Nomenclature}

\begin{tabular}{lcl}
\large{\bf VM}  & & \large{Virtual Machine} \\
\large{\bf KVM}  & & \large{Kernel-based Virtual Machine} \\
\large{\bf OS}   & & \large{Operating System}        \\
\large{\bf VMI}  & & \large{Virtual Machine Introspection} \\
\large{\bf CPU}  & & \large{Central Processing Unit} \\
\large{\bf AMD-V}  & & \large{Advanced Micro Devices Virtualization} \\
\large{\bf VT-x}  & & \large{Intel Virtualization Extension} \\
\large{\bf MSR}  & & \large{Model Specific Register} \\
\large{\bf VMM}  & & \large{Virtual Machine Monitor, analogous to a hypervisor} \\
\large{\bf EFER}  & & \large{Extended Feature Enable Register} \\
\large{\bf eBPF}  & & \large{Extended Berkeley Packet Filter} \\
\large{\bf VMI}  & & \large{Virtual Machine Introspection} \\
\large{\bf API}  & & \large{Application Programming Interface} \\
\large{\bf IDS}  & & \large{Intrusion Detection System} \\
\end{tabular}














% Introduction


\newpage
\chapter{Introduction}
\pagenumbering{arabic}


{\large
Cloud computing is a modern method for delivering computing power, storage services, databases, networking, analytics, artificial intelligence, and software applications over the internet (the cloud). Organizations of every type, size, and industry are using the cloud for a wide variety of use cases, such as data backup, disaster recovery, email, virtual desktops, software development, testing, big data analytics, and web applications. For example, healthcare companies are using the cloud to store patient records in databases. Financial service companies are using the cloud to power real-time fraud detection and prevention. And finally, video game companies are using the cloud to deliver online game services to millions of players around the world.
\newline
}

{\large
The existance of cloud computing can be attributed to virtualization. Virtualization is a technology that makes it possible for multiple different operating systems (OSs) to run concurrently, and in an isolated environment on the same hardware. Virtualization makes use of a machines hardware to support the software that creates and manages virtual machines (VMs). A VM is a virtual environment that provides the functionality of a physical computer by using its own virtual central processing unit (CPU), memory, network interface, and storage. The software that creates and manages VMs is formally called a hypervisor or virtual machine monitor (VMM). The virtualization marketplace is comprised of four notable hypervisors: (1) VMWare, (2) Xen, (3) Kernel-based Virtual Machine (KVM), and (4) Hyper-V. The operating system running a hypervisor is called the host OS, while the VM that uses the hypervisors resources is called the guest OS.
\newline
}

{\large
While virtualization technology can be sourced back to the 1960s, it wasn’t widely adopted until the early 2000s due to hardware limitations. The fundamental reason for introducing a hypervisor layer on a modern machine is that without one, only one operating system would be able to run at a given time. This constraint often led to wasted resources, as a single OS infrequently utilized a modern hardware’s full capacity. In other words, the computing capacity of a modern CPU is so large, that under most workloads, it is difficult for a single OS to efficiently use all of its resources at a given time. Hypervisors address this constraint by allowing all of a system’s resources to be utilized by distributing them over several VMs. This allows users to switch between one machine, many operating systems, and multiple applications at their discretion.
\newline
}

{\large 
Due to exposure to the Internet, VMs represent a first point-of-target for security attackers who want to gain access into the virtualization environment [3]. A virtual machine isolated from the Internet may behave consistently over time. However, a VM exposed to the Internet is a new system every day, as the rest of the Internet changes around it [2]. As such, the role of a VM is highly security critical and its priority should be to maintain confidentially, integrity, authorization, availability, and accountability throughout its existance. The successful exploitation of a VM can result in a complete breach of isolation between clients, resulting in the failure to meet one or more of the aforementioned priorities. For example, the successful exploitation of a VM can result in the loss of availability of client services due to denial-of-service attacks, non-public information becoming accessible to unauthorized parties, data, software or hardware being altered by unauthorized parties, and the successful repudiation of a previous action by a principal. For these reasons, effective methodologies for monitoring VMs is required.
\newline
}

{\large 
In this thesis, we present Frail, a KVM hypervisor and Intel VT-x exclusive Hypervisor-based virtual machine introspection (VMI) framework that enhances the capabilities of existing, and related VMIs. In computing, VMI is a technique for monitoring the runtime state of a virtual machine [4]. Frail is a VMI for (1) tracing KVM guest system calls, (2) monitoring malicious anomalies, and (3) responding to those malicious anomalies from the hypervisr level. Our framework is implemented using a combination of existing software and our own software. Firstly, it utilizes Extended Berkeley Packet Filter (eBPF) to safely extract both KVM guest system calls and the corresponding process that requested the system call. Secondly, it uses Anil Somayaji's pH [2] implementation of sequences of system calls to detect malicious anomalies. Lastly, we make use of our own software to respond to the observed malicious anomalies by slowing down or terminating the guest process responsible for the malicious anomaly. To our knowledge, Frail is the second VMI-based system that is intended to support all three system call mechanisms provided by the Intel x86 architecture, and has been proven to work for Linux 64-bit systems.
\newline
}





\section{Research Questions}

{\large
In this thesis, we consider the following research questions:
\newline
}

{\large
\textbf{Research Question 1}: As KVM is defined as a type 1 hypervisor, instructions are sent directly to the CPU. Can we change the route of system calls so that they are delivered and emulated at the hypervisor level?
\newline
}

{\large
\textbf{Research Question 2}: Can we bridge the semantic gap placed by the KVM hypervisor to successfully retrieve KVM guest system calls from the host? Can we utilize KVM guest system calls and sequences of system calls to successfully detect malicious anomalies in real-time without without hindering usability of the guest?
\newline
}

{\large
\textbf{Research Question 3}: What improvements to the Linux tracepoints API would be required for eBPF to successfully trace KVM guest system calls and the corressponding process that requested the system call?
\newline
}








\section{Motivation}
{\large
Current computer systems have no default general-purpose mechanism for detecting and responding to malicious anomalies within KVM VMs. As our computer systems grow increasingly complex, so too does it become more difficult to gauge precisely what they are doing at any given moment. Modern computers are often running hundreds, if not thousands of processes at any given time, the vast majority of which are running silently in the background. As a result, users often have a very limited notion of what exactly is happening on their systems, especially beyond that which they can actually see on their screens. An unfortunate corollary to this observation is that users also have no way of knowing whether their system may be misbehaving at a given moment. For this reason, we cannot rely on users to detect and respond to malicious anomalies. If users are not good candidates for adequately monitoring our VMs for malicious anomalies, VMs should be programmed to watch over themselves through the hypervisor level. Due to VMs being highly security critical, we have turned to VMI to provide the necessary tools to help trace, monitor and respond to malicious anomalies found within KVM VMs.
\newline
}




\subsection{Why Design a New VMI?}

{\large
The problem of protecting VMs dates back to 2003, when Garfinkel and Rosenblum first introduced the concept of VMI as a hypervisor-level Intrusion Detection System (IDS), which combined the advantages of both network-based and host-based IDS [10][2]. Since then, widespread research and deployment of VMs has led to an abundent creation of tools and frameworks, some more practical than others, for the purpose of limiting the damage that untrusted software can do to VMs. These are covered in more depth in Chapter 2. For now, we focus on why it might be necessary to design and implement yet another KVM VMI framework even though many of them already exist.
\newline
}


{\large
At the time of writing this thesis, to our knowledge, there is one relevant KVM VMI to our thesis named Nitro, for system call tracing and monitoring, which was intended, implemented, and proven to support Windows, Linux, 32-bit, and 64-bit environments. The problem with Nitro is that at the time of this writing, it is over 11 years old, and has not been updated in over 6 years. For this reason, it is no longer compatiable with any Linux 32-bit, and 64-bit environments, and is not compatiable with newer Windows desktop versions. As of writing this, Nitro only supports Windows XP x64 and Windows 7 x64, making it completely useless as Windows XP is a discontinued OS that is now over 21 years old and consists of only 0.39\% of worldwide Windows desktop version market share. Similarly, Windows 7 is 13 years old, and consists of only 9.6\% of worldwide Windows desktop version market share. There is a fundamental problem with the state of many existing VMI's like Nitro: When the codebase OSs change or kernels change, they are unable to solve the problem for which they were originally designed to solve: to trace and monitor VMs to protect the VM from being compromised in the event of a successful attack [3]. One of the primary reasons for this is that current VMIs were designed in such a way that compromised compatibility with subsequent versions of the OSs with which they were originally compatible with. To solve the problem of Nitro's incapabilities, we seek to design a spiritual successsor to Nitro that is intended to provide a VMI without sacrificing compatibility with subsequent versions of the Linux kernel. We will discuss this further in the "Contributions" section and "Background" chapter.
\newline
}












\subsection{Why Design a Hypervisor-Based Virtual Machine Introspection?}

{\large
A VMI system can either be placed in each VM that requires monitoring (guest-based monitoring), or it can be placed on the hypervisor level outside of any VM (Hypervisor-based monitoring). In this section, we discuss our motivations for designing and implementing an Hypervisor-based VMI by analyzing the advantages and disadvantages of both Hypervisor-based and guest-based VMI's.
\newline \newline
\textbf{Hypervisor-based VMI's}
\newline
}


{\large
Hypervisor-based monitors offer four key advantages over traditional guest-based VMI's: (1) isolation, (2) inspection, (3) interposition, and (4) deployability [8].
\newline
}

{\large
Isolation refers to the property that Hypervisor-based VMI's are less susceptible to tampering [8]. Unlike guest-based VMI's, Hypervisor-based VMI's run at a higher privlige level than guest OS'. For this reason, the Hypervisor-based VMI's are isolated from the guest OS, which makes them more tamper-resistant and inaccessible by the guest OS compared to guest-based VMI's [7]. By isloating a VMI from a guest OS, it allows the VMI to be immune from attacks that originate in the guest, even if the VMI is actively monitoring the attacked guest [7].
\newline
}

{\large
The second key advantage, which is inspection, refers to the property that allows the VMI to examine the entire state of the guest OS while continuing to be isolated [8]. Hypervisor-based VMI's run one layer below all the guest OSs, and on the same layer of the hypervisor. For this reason, the VMI is capable of efficiently having a complete view of all guest OS states (CPU registers, memory, devices, and more) [7]. A VMI isolated from the VM also offers the advantage for constant and consistent view of the system state, even if a VM is in a paused state. In contrast, if a guest-based VMI  was used, it would not be isolated, and thus, the VMI would stop executing when a VM goes into a paused state. 
\newline
}


{\large
In terms of our VMI, isolation allows us to efficiently, and consistently trace KVM guest system calls and the corressponding process that requested the system call, without having to rely on a possibly compromised VM for this information [8]. The third key advantage is interposition, which is the the ability to inject operations into a running VM based on certain conditions. Due to the interposition of the hypervisor and the VMI, the VMI is capable of modifying any of the states of the guest OS and interfering with every activity of the guest. In terms of our VMI, interposition makes it easy to respond to the observed malicious anomalies by slowing down the process responsible for the malicious anomaly [7]. Hypervisor-based VMI's are very adaptable due to their existance in only the host OS. 
\newline
}


{\large
Deployability of a VMI refers to the ease with which it can be taken from development to production. It can be measured in terms of the complexity of the number of discrete steps required for deploying code from a development to the production environment. To deploy an Hypervisor-based VMI at the hypervisor layer, no guest has to be modified to accomodate for the VMI's deployment. For example, we do not have to make an account for any guest. We also do not need to install the software inside the guest. Instead, we simply need to execute our VMI on the host during its runtime, which will not disrupt any services in neither the host or guest.
\newline
}


{\large
Although guest-based systems have been very successful with systems like [insert guest-based vmi here], they are more susceptible to three types of threats: (1) privlidge escalation, (2) spoofing, and (3) tampering [8]. 
\newline
}

{\large
guest-based VMI's are not isolated as well as hypervisor-based VMI's, because they are executed on the same privlidge level as the VM(s) that they are protecting [9]. For this reason, malicious software (malware), such as kernel rootkits can escalate privlidge, and allow an attacker with unauthorized access to a VMI, or software that a VMI depends on. And if successful, an attacker can tamper with all the components of a VMI. For example, attackers can tamper with the tracing software that collects system call information and/or the corresponding process that requested the system call. For instance, since our VMI depends on hooking specific kernel functions, attackers can modify the relevant symbols within the symbol table to bypass VMI properties. They can also tamper with the software that handles sequences of system calls, which is the monitoring tool that enforces our VMI's security policy. Finally, attackers can tamper with the software that the VMI depends on like logs and insert them with false data. guest-based VMI's usually have to trust the entire guest OS kernel, which tends to have a huge code base. However, Hypervisor-based often only needs to trust the underlying hypervisor, which has a smaller code base. For example, the KVM hypervisor has less than one twelfth the number of lines of code than the Linux kernel; this smaller attack surface leads to fewer vulnerabilities. Also, if the VMI is deployed using a kernel module in kernel space or a normal process in user space, attackers can simply remove or shut down the monitoring module or process. As long as an attack results in the VMI to continue its normal execution (e.g., no crashes), the VMI system can successfully generate a false pretense to mislead the VMI that a VM state is not malicious, when in fact it is.
\newline
}


{\large
guest-based VMI's have many two fundamental advantages like (1) rich abstractions, and (2) fast speed. There are plenty of abstractions for guest-based monitors from which to extract the OS and process state. They can use critical kernel variables and functions, system call and process information provided by the guest OS. At an even higher level of abstraction, they can also use the available guest-based security tools to extract the state. With guest-based VMI's, we are able to trivially intercept system calls, and processess, and inspect their sequences.
\newline
}



{\large
All of the elements of guest-based VMI's can be executed faster than Hypervisor-based VMI's because tracing system calls, monitoring for anomalies, and responding to anomalies do not require trapping to the hypervisor. However, we believe that the disadvantages of guest-based VMI's outweigh the advantages. In other words, the security of a VMI and the VM's that intended to be protected by the VMI are more important than rich abstractions and speed that that guest-based VMI's provide. For that reason, we have designed and implemented an Hypervisor-based VMI.
\newline
}














\subsection{Why eBPF?}















\subsection{Why Utilize System Calls for Introspection?}

{\large
If there exists an application programming interface that maps information from the guest to the hypervisor level, then a VMI can collect an audit trail, and use that information in a variety of ways to maintain the stability and security of a VM. For example, it is possible to build a VMI system that utilizes memory of guest processes, guest processor instructions, a given guest user’s keystrokes or commands, and the guest systems resource usage. In the ideal case, the VMI can analyze the audit trail, flag any unusual, anomalous, or prohibited behavior, and then initiate a response immediately with a high success rate, and without hindering the overall performance of the virtualization environment. In an non ideal case, the VMI does the opposite of the ideal case. In the worst case, the methods and type of data that a VMI collects can only be achieved by halting the guest while introspection takes place (pause-and-introspect), but do not create a separate memory image and access guest memory contents directly. This significantely hinders the overall performance the virtualization environment. Another worst case senario approach for VMI's depending on the type of data they collect is examining their data trail during off-peak hours, so the guest VM can constantly stay active. With this type of VMI implementation, there is a chance, that a particularly successful intruder could tamper the trail and hide the intrusion. For this reason, a computationally fast realtime method and best trace subject is useful. What follows is an explanation of what a system call is, and an explanation of why the system call interface has several special properties that make it a good choice for monitoring program behavior for security violations.
\newline
}

{\large
 
On UNIX and UNIX-like systems, user programs do not have direct access to hardware resources; instead, one program, called the kernel, runs with full access to the hardware, and regular programs must ask the kernel to perform tasks on their behalf. Running instances of a program are known as processes.

The system call is a request by a process for a service from the kernel. The service is generally something that only the kernel has the privilege to perform. For example, when a process wants additional memory, or when it wants to access the network,
disk, or other I/O devices, it requests these resources from the kernel through system calls. Such calls normally takes the form of a software interrupt instruction that switches the processor into a special supervisor mode and invokes the kernel’s system call dispatch routine. If a requested system call is allowed, the kernel performs the requested operation and then returns control either to the requesting process or to
another process that is ready to run.


Hence, system calls play a very important role in events such as context switching, memory access, page table access and interrupt handling. With the exception of system calls, processes are confined to their own address space. If a process is to damage anything outside of this space, such as other programs, files, or other networked machines, it must do so via the system call interface. Unusual system calls indicate that a process is interacting with the kernel in potentially dangerous ways. Interfering with these calls can help prevent damage, and help maintain the stability and security of a VM. previously created VMI's have utilized system calls to passively flag any unusual, anomalous, or prohibited behavior with a high success rate, without hindering the overall performance of the virtualization environment, and while keeping the guest OS active.
\newline
}




























































\subsection{Why Sequences of System Calls?}

Instead of system call sequences, a neural network implementation is a modern
approach to solving the problem of detecting malicious processes. Although system
call sequences requires a less complex implementation than that of a neural network
implementation, we believe complexity does not equate to better. Our motivation for
using an implementation based on system call sequences is because we believe it is
still effective in detecting and respeonding to malicious processes.

















\section{Problem}
\subsection{The Semantic Gap Problem}
When designing computer monitoring systems, one goal has always been to have a complete view of the
monitored target and at the same time stealthily protect the monitor itself. One way to achieve this is to
use hypervisor-based, or more generally out of virtual machine (VM)-based, monitoring. There are, however,
challenges that limit the use of this mechanism; the most significant of these is the semantic gap problem.



In order to leverage the full potential that VMI provides, identifying and
isolating the relevant guest operating system (OS) state information becomes
crucial. This process requires some semantic knowledge about the guest and is
referred to as the semantic gap issue [3]. Bridging this semantic gap has been
classified into three fundamental view generation patterns [12]. One of these
patterns relies on knowledge of the hardware architecture to derive semantic
information about the guest OS. Making use of the hardware architecture allows
one to construct mechanisms that are resistant to evasion attempts through a
method called hardware rooting [13]. This makes hardware-based information
extraction particularly interesting for security approaches that are intended to
detect malicious activity within a monitored VM.















\section{Approaching the Problem}

















\section{Contributions}


Since our proposed solution is based on KVM, it requires modification of the kernel during its runtime, and can be dynamically loaded at runtime. This means that we can 





While introspection has many applications, it is fundamentally limited because it can only perform passive monitoring. Therefore, introspection alone is not sufficient for applications that rely on active monitoring, such as anti-virus
tools and host-based intrusion prevention systems.











\section{Thesis Organization}

In this thesis, we will examine the design and implementation of our VMI, explore its security implications on the KVM hypervisor and its guests, and explore its impact on system performance.




\chapter{Related work}



\chapter{Background}






\section{Virtual Machine Introspection}

VMI describes the method of monitoring and analyzing the state of a virtual machine from the hypervisor level [Nitro]. 


\section{Hypervisor}

{\large
There are two ways a hypervisor can virtualize a machine:

A hypervisor runs Guest OS instructions either directly on the host's CPU, or on the host OS. In both scenarios, 
the goal of a hypervisor is to provide a software-controlled layer that resembles the host hardware. Hypervisors 
can be classified into two types that are dependent on how they to runs Guest OS instructions.
\newline
}

{\large 
(1) Type 1 (bare metal) hypervisors, which runs Guest OS instructions directly on the host’s hardware in order to control
the hardware and monitor the guest OS. Typical examples of such hypervisors include Xen, VMware ESX, and Microsoft Hyper-V.
\newline
}

{\large
(2) Type 2 (hosted) hypervisors, which run within a traditional OS. In other words, a
hosted hypervisor adds a distinct software layer atop the host OS, and the guest OS becomes a third
software layer above the hardware. Well-known examples of type 2 hypervisors include KVM, 
VMware Workstation, VirtualBox, and QEMU.
\newline
}

{\large
Although the preceding type 1 and type 2 hypervisor classification has been widely
accepted, it is not clear it insufficiently differentiates among hypervisors of the same type (e.g., KVM vs. QEMU).


KVM is not a clear case as it could be categorized as either one. The KVM kernel module turns Linux kernel into a type 1 bare-metal hypervisor, while the overall system could be categorized to type 2 because the host OS is still fully functional and the other VM's are standard Linux processes from its perspective.


There-fore, based on how the virtualization gets designed (hardware vs. software) and the guest OS and its 
application code is executed, we can have another type of classification of hypervisors that will be used 
throughout this thesis:
\newline
}

{\large
(1) Native hypervisors that directly push the guest code to execute natively on the
hardware using hardware virtualization.
\newline
}


{\large
(2) Emulation hypervisors that translate each guest instruction for an emulated exe-
cution using software virtualization.
\newline
}

{\large
Examples of native hypervisors include Xen, KVM, VMware ESX, and Microsoft HyperV, and emulation hypervisors include QEMU, Bochs, and the very early versions of
VMware-Workstation and VirtualBox (note that recent VMware-Workstation and VirtualBox are able to execute the guest OS code natively). Since there is no binary code
translation involved, native hypervisor runs much faster than emulation hypervisor.

In this thesis, we will be solely on the KVM VM.








Hardware-assisted




}













\section{Intel Virtualization Extention (VT-X)}

{\large
Protection rings is a mechanism to protect data and functionality from faults (by improving fault tolerance) and malicious behaviour (by providing computer security). It is designed to have a hierarchical design that separates and limits the interaction between the user space and kernel space within and OS. It's purpose is to provide fault protection and tolerance among computer users, components, applications, and processes.
\newline
}

% Ring Figure
{\large
\definecolor{grad_ring_zero}{HTML}{7f469e}
\definecolor{grad_ring_one}{HTML}{75499e}
\definecolor{grad_ring_two}{HTML}{6b4b9e}
\definecolor{grad_ring_three}{HTML}{614d9c}
\definecolor{grad_guest_ring_zeros}{HTML}{5a4e9a}
\definecolor{grad_guest_ring_one}{HTML}{485095}
\definecolor{grad_guest_ring_two}{HTML}{39518e}
\definecolor{grad_guest_ring_three}{HTML}{2c5186}
\begin{figure}[!h]
\caption{Illustration of the x86 Protection Ring of a VM}
\begin{center}
\begin{tikzpicture}

\draw[fill=grad_guest_ring_three, very thick] (-1,0) 
circle (8.5) node [align=center, text=black] at (-1,7.95) {\textbf{Guest Ring 3} \\ \textbf{Guest User Space}};

\draw[fill=grad_guest_ring_two, very thick] (-1,0) 
circle (7.5) node [text=black] at (-1,6.95) {\textbf{Guest Ring 2}};

\draw[fill=grad_guest_ring_one, very thick] (-1,0) 
circle (6.5) node [text=black] at (-1,6) {\textbf{Guest Ring 1}};

\draw[fill=grad_guest_ring_zeros, very thick] (-1,0) 
circle (5.5) node [align=center, text=black] at (-1,5) {\textbf{Guest Ring 0} \\ \textbf{Guest Kernel}};

\draw[fill=grad_ring_three, very thick] (-1,0) 
circle (4.5) node [align=center, text=black] at (-1,3.93) {\textbf{Host Ring 3} \\ \textbf{Host User Space}};

\draw[fill=grad_ring_two, very thick] (-1,0)
circle (3.5) node [text=black] at (-1,2.9) {\textbf{Host Ring 2}};

\draw[fill=grad_ring_one, very thick] (-1,0) 
circle (2.5) node [align=center, text=black] at (-1,1.85) {\textbf{Host Ring 1}};

\draw[fill=grad_ring_zero, very thick] (-1,0) 
circle (1.5) node [align=center, text=black] at (-1,0) {\textbf{Host Ring 0} \\ \textbf{Host Kernel} \\ \textbf{KVM} \\ \textbf{Frail VMI}};

\end{tikzpicture}
\end{center}
\end{figure}
}

{\large
Ring 3 is the least privileged, and is where normal user processes execute. In this ring 3, you cannot execute privileged instructions. Ring 0 is the most privileged ring that allows the execution of any instruction. In normal operation, the kernel runs in ring 0. Ring 1 and 2 are not used by any current operating system. However, hypervisors are free to use them as needed [6]. As shown in Fig. 1.1, the KVM hypervisor is kept in kernel mode (ring 0), the applications in user mode (ring 3), and the guest OS in a layer of intermediate privilege (ring 1). As a result, the kernel is privileged relative to the user processes and any attempt to access kernel memory from the guest Os program leads to an access violation. At the same time, the guest operating system’s privileged instructions trap to the hypervisor. The hypervisor then performs the privileged instruction(s) on the guest OS' behalf.
\newline
}




\section{The Kernel Virtual Machine Hypervisor}


{\large 
Kernel-based Virtual Machine (KVM) is a hypervisor that is implemented as a Linux kernel module that allows the kernel to function as a hypervisor. It was merged into the mainline Linux kernel in version 2.6.20, which was released on February 5, 2007. KVM requires a CPU with hardware virtualization extensions, such as Intel VT-x or AMD-V. While working with KVM, we will only be focusing on Intel VT-x hardware virtualization.
\newline
}



\subsection{Model Specific Registers}
\subsection{VMCS}
\subsection{VM ENTRY Context Switch}
\subsection{VM EXIT Context Switch}
\section{QEMU}
\section{System Calls}
\section{Virtual Machine Introspection}

In general, a security monitoring system can be defined as
M(S, P) → {True,False}, (1)
where M denotes the security enforcing mechanism, S denotes the current system
state, and P denotes the predefined policy. If the current state S satisfies the security
policy P, then it is in a secure state (True), and if M is an online mechanism, it can
allow continued execution. Otherwise, it is insecure (False); an attack1 is detected, and
M can halt the execution (for prevention) or report that there is an attack instance.
For example, in an antivirus system, S can denote the current memory and disk state,
and P the signatures of viruses; if M identifies that there is any running process or
suspicious file having one of the signatures defined in P, the antivirus will raise an
alarm. In a system call–based intrusion detection system, S can denote the current
system call and P can denote the correct state machines for S; if M identifies that
S deviates from P, then it can raise an intrusion alert.


\section{eBPF}
\section{The Linux Kernel Tracepoint API}
\section{pH-based Sequences of System Call}



{\large


There are 12 projects that use the guest-assisted approach. The pioneer
work, LARES [Payne et al. 2008], inserts hooks in a guest VM and protects its guest com-
ponent by using the hypervisor for memory isolation with the goal of supporting active
monitoring. Unlike passive monitoring, active monitoring requires the interposition of kernel events. As a result, it requires the monitoring code to be executed inside the guest OS, which is why it essentially leads to the solution of inserting certain hooks inside the guest VM. The hooks are used to trigger events that can notify the hypervisor
or redirect execution to an external VM. More specifically, LARES design involves three
components: a guest component, a secure VM, and a hypervisor. The hypervisor helps
to protect the guest VM component by memory isolation and acts as the communica-
tion component between the guest VM and the secure VM. The secure VM is used to
analyze the events and take actions necessary to prevent attacks.


}

\section{Nitro: Hardware-Based System Call Tracing for Virtual Machines}

\chapter{Designing Frail}
\chapter{Implementing Frail}

\section{User Space Component}
\section{Kernel Space Component}
\subsection{Custom Linux Kernel Tracepoint}
\subsection{Kernel Module}
\section{Tracing Processess}
\section{Proof of Tracability of all KVM Guest System Calls}

\chapter{Threat Model of Frail}


\chapter{Future Work}

\chapter{Conclusion}

\chapter{References}

{\large
https://dl.acm.org/doi/pdf/10.1145/361011.361073 [1]
https://people.scs.carleton.ca/~soma/pubs/soma-diss.pdf [2]
https://dl.acm.org/doi/pdf/10.1145/2659651.2659710 [3]
https://doi.org/10.1007/978-1-4419-5906-5\_647 [4]
https://doi.org/10.1007/978-3-642-25141-2\_7 [5]
modern operating systems andrew s. tanenbaum [6]
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=7299979 [7]
https://dl.acm.org/doi/pdf/10.1145/1655148.1655150 [8]
https://dl.acm.org/doi/10.1145/2775111 [9]
https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=7299979 [10]
}
\end{spacing}
\end{document}